{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California Housing Prices: Regresión con PyTorch y Weights & Biases\n",
    "\n",
    "En esta notebook, implementaremos un modelo de regresión para predecir valores medianos de casas basado en un conjunto de datos de características demográficas y geográficas. Además, presentaremos los aspectos básicos del entrenamiento de modelos en PyTorch y utilizaremos la herramienta Weights & Biases (W&B) para el seguimiento y visualización de los experimentos.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Objetivos\n",
    "\n",
    "1. **Implementar un modelo de regresión** utilizando PyTorch.\n",
    "2. **Configurar y utilizar W&B** para el seguimiento de métricas y visualización de resultados.\n",
    "\n",
    "### Contenido\n",
    "\n",
    "1. Configuración de bibliotecas y semillas para reproducibilidad.\n",
    "2. Carga y exploración del dataset `housing.csv`.\n",
    "3. Preparación de datos y división en conjuntos de entrenamiento y prueba.\n",
    "4. Definición y entrenamiento de un modelo de regresión en PyTorch.\n",
    "5. Integración de W&B para el seguimiento de experimentos y visualización de métricas.\n",
    "\n",
    "### Sobre el conjunto de datos\n",
    "\n",
    "Los datos se refieren a las casas que se encuentran en un determinado distrito de California y algunas estadísticas resumidas sobre ellas basadas en los datos del censo de 1990. Hay que tener en cuenta que los datos no están depurados, por lo que se requieren algunos pasos de preprocesamiento. El objetivo es predecir el valor medio de las casas para los distritos de California, por lo que se trata de un problema de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijamos la semilla para que los resultados sean reproducibles\n",
    "SEED = 34\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando cpu\n",
      "Usando 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# definimos el dispositivo que vamos a usar\n",
    "DEVICE = \"cpu\"  # por defecto, usamos la CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"  # si hay GPU, usamos la GPU\n",
    "\n",
    "print(f\"Usando {DEVICE}\")\n",
    "\n",
    "NUM_WORKERS = 0 # Win y MacOS pueden tener problemas con múltiples workers\n",
    "if sys.platform == 'linux':\n",
    "    NUM_WORKERS = 4  # numero de workers para cargar los datos (depende de cada caso)\n",
    "\n",
    "print(f\"Usando {NUM_WORKERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 1024  # tamaño del batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos + Exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 20640\n",
      "\n",
      ":Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      ":Attribute Information:\n",
      "    - MedInc        median income in block group\n",
      "    - HouseAge      median house age in block group\n",
      "    - AveRooms      average number of rooms per household\n",
      "    - AveBedrms     average number of bedrooms per household\n",
      "    - Population    block group population\n",
      "    - AveOccup      average number of household members\n",
      "    - Latitude      block group latitude\n",
      "    - Longitude     block group longitude\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "A household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surprisingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. rubric:: References\n",
      "\n",
      "- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "  Statistics and Probability Letters, 33:291-297, 1997.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "california_housing = fetch_california_housing(as_frame=True)\n",
    "target_column_name = california_housing.target_names[0]  # 'MedHouseVal'\n",
    "print(california_housing.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  MedHouseVal  \n",
       "0    -122.23        4.526  \n",
       "1    -122.22        3.585  \n",
       "2    -122.24        3.521  \n",
       "3    -122.25        3.413  \n",
       "4    -122.25        3.422  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = california_housing.frame  # Convertimos el dataset en un DataFrame de pandas\n",
    "df.head()  # Mostramos las primeras filas del DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.870671</td>\n",
       "      <td>28.639486</td>\n",
       "      <td>5.429000</td>\n",
       "      <td>1.096675</td>\n",
       "      <td>1425.476744</td>\n",
       "      <td>3.070655</td>\n",
       "      <td>35.631861</td>\n",
       "      <td>-119.569704</td>\n",
       "      <td>2.068558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.899822</td>\n",
       "      <td>12.585558</td>\n",
       "      <td>2.474173</td>\n",
       "      <td>0.473911</td>\n",
       "      <td>1132.462122</td>\n",
       "      <td>10.386050</td>\n",
       "      <td>2.135952</td>\n",
       "      <td>2.003532</td>\n",
       "      <td>1.153956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.499900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>-124.350000</td>\n",
       "      <td>0.149990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.563400</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.440716</td>\n",
       "      <td>1.006079</td>\n",
       "      <td>787.000000</td>\n",
       "      <td>2.429741</td>\n",
       "      <td>33.930000</td>\n",
       "      <td>-121.800000</td>\n",
       "      <td>1.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.534800</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>5.229129</td>\n",
       "      <td>1.048780</td>\n",
       "      <td>1166.000000</td>\n",
       "      <td>2.818116</td>\n",
       "      <td>34.260000</td>\n",
       "      <td>-118.490000</td>\n",
       "      <td>1.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.743250</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>6.052381</td>\n",
       "      <td>1.099526</td>\n",
       "      <td>1725.000000</td>\n",
       "      <td>3.282261</td>\n",
       "      <td>37.710000</td>\n",
       "      <td>-118.010000</td>\n",
       "      <td>2.647250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.000100</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>141.909091</td>\n",
       "      <td>34.066667</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>1243.333333</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>-114.310000</td>\n",
       "      <td>5.000010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
       "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
       "mean       3.870671     28.639486      5.429000      1.096675   1425.476744   \n",
       "std        1.899822     12.585558      2.474173      0.473911   1132.462122   \n",
       "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
       "25%        2.563400     18.000000      4.440716      1.006079    787.000000   \n",
       "50%        3.534800     29.000000      5.229129      1.048780   1166.000000   \n",
       "75%        4.743250     37.000000      6.052381      1.099526   1725.000000   \n",
       "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
       "\n",
       "           AveOccup      Latitude     Longitude   MedHouseVal  \n",
       "count  20640.000000  20640.000000  20640.000000  20640.000000  \n",
       "mean       3.070655     35.631861   -119.569704      2.068558  \n",
       "std       10.386050      2.135952      2.003532      1.153956  \n",
       "min        0.692308     32.540000   -124.350000      0.149990  \n",
       "25%        2.429741     33.930000   -121.800000      1.196000  \n",
       "50%        2.818116     34.260000   -118.490000      1.797000  \n",
       "75%        3.282261     37.710000   -118.010000      2.647250  \n",
       "max     1243.333333     41.950000   -114.310000      5.000010  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()  # Mostramos un resumen de las estadísticas del DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MedInc         0\n",
       "HouseAge       0\n",
       "AveRooms       0\n",
       "AveBedrms      0\n",
       "Population     0\n",
       "AveOccup       0\n",
       "Latitude       0\n",
       "Longitude      0\n",
       "MedHouseVal    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chequear si hay valores nulos\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afortunadamente, el conjunto de datos no tiene valores faltantes, por lo que podemos ignorar la imputación de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  \\\n",
      "MedInc       1.000000 -0.119034  0.326895  -0.062040    0.004834  0.018766   \n",
      "HouseAge    -0.119034  1.000000 -0.153277  -0.077747   -0.296244  0.013191   \n",
      "AveRooms     0.326895 -0.153277  1.000000   0.847621   -0.072213 -0.004852   \n",
      "AveBedrms   -0.062040 -0.077747  0.847621   1.000000   -0.066197 -0.006181   \n",
      "Population   0.004834 -0.296244 -0.072213  -0.066197    1.000000  0.069863   \n",
      "AveOccup     0.018766  0.013191 -0.004852  -0.006181    0.069863  1.000000   \n",
      "Latitude    -0.079809  0.011173  0.106389   0.069721   -0.108785  0.002366   \n",
      "Longitude   -0.015176 -0.108197 -0.027540   0.013344    0.099773  0.002476   \n",
      "MedHouseVal  0.688075  0.105623  0.151948  -0.046701   -0.024650 -0.023737   \n",
      "\n",
      "             Latitude  Longitude  MedHouseVal  \n",
      "MedInc      -0.079809  -0.015176     0.688075  \n",
      "HouseAge     0.011173  -0.108197     0.105623  \n",
      "AveRooms     0.106389  -0.027540     0.151948  \n",
      "AveBedrms    0.069721   0.013344    -0.046701  \n",
      "Population  -0.108785   0.099773    -0.024650  \n",
      "AveOccup     0.002366   0.002476    -0.023737  \n",
      "Latitude     1.000000  -0.924664    -0.144160  \n",
      "Longitude   -0.924664   1.000000    -0.045967  \n",
      "MedHouseVal -0.144160  -0.045967     1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Crear la matriz de correlación\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Mostrar la matriz de correlación\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este momento vemos que existen relaciones fuertes entre algunas variables como `MedInc`, `HouseAge`, y `AveRooms`  con `MedHouseVal`, lo cual suena lógico. Podríamos descartar algunas variables que no aportan mucho valor al modelo, pero por simplicidad, las mantendremos todas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos\n",
    "\n",
    "Para el preprocesamiento de datos nos vamos a limitar a la normalización de los datos. La normalización es una técnica común en el aprendizaje automático para mejorar la convergencia y la estabilidad del modelo. En este caso, normalizaremos todas las variables a un rango de 0 a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.064000e+04</td>\n",
       "      <td>2.064000e+04</td>\n",
       "      <td>2.064000e+04</td>\n",
       "      <td>2.064000e+04</td>\n",
       "      <td>2.064000e+04</td>\n",
       "      <td>2.064000e+04</td>\n",
       "      <td>2.064000e+04</td>\n",
       "      <td>2.064000e+04</td>\n",
       "      <td>20640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.406467e-17</td>\n",
       "      <td>1.101617e-17</td>\n",
       "      <td>6.885104e-17</td>\n",
       "      <td>-1.018995e-16</td>\n",
       "      <td>-1.514723e-17</td>\n",
       "      <td>2.754042e-18</td>\n",
       "      <td>-1.035520e-15</td>\n",
       "      <td>-8.526513e-15</td>\n",
       "      <td>2.068558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.153956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.774256e+00</td>\n",
       "      <td>-2.196127e+00</td>\n",
       "      <td>-1.852274e+00</td>\n",
       "      <td>-1.610729e+00</td>\n",
       "      <td>-1.256092e+00</td>\n",
       "      <td>-2.289944e-01</td>\n",
       "      <td>-1.447533e+00</td>\n",
       "      <td>-2.385935e+00</td>\n",
       "      <td>0.149990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.881019e-01</td>\n",
       "      <td>-8.453727e-01</td>\n",
       "      <td>-3.994399e-01</td>\n",
       "      <td>-1.911670e-01</td>\n",
       "      <td>-5.637952e-01</td>\n",
       "      <td>-6.170912e-02</td>\n",
       "      <td>-7.967694e-01</td>\n",
       "      <td>-1.113182e+00</td>\n",
       "      <td>1.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.767908e-01</td>\n",
       "      <td>2.864502e-02</td>\n",
       "      <td>-8.078293e-02</td>\n",
       "      <td>-1.010626e-01</td>\n",
       "      <td>-2.291262e-01</td>\n",
       "      <td>-2.431526e-02</td>\n",
       "      <td>-6.422715e-01</td>\n",
       "      <td>5.389006e-01</td>\n",
       "      <td>1.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.592952e-01</td>\n",
       "      <td>6.642943e-01</td>\n",
       "      <td>2.519554e-01</td>\n",
       "      <td>6.015724e-03</td>\n",
       "      <td>2.644885e-01</td>\n",
       "      <td>2.037404e-02</td>\n",
       "      <td>9.729330e-01</td>\n",
       "      <td>7.784775e-01</td>\n",
       "      <td>2.647250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.858144e+00</td>\n",
       "      <td>1.856137e+00</td>\n",
       "      <td>5.516190e+01</td>\n",
       "      <td>6.957003e+01</td>\n",
       "      <td>3.024960e+01</td>\n",
       "      <td>1.194162e+02</td>\n",
       "      <td>2.957996e+00</td>\n",
       "      <td>2.625216e+00</td>\n",
       "      <td>5.000010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
       "count  2.064000e+04  2.064000e+04  2.064000e+04  2.064000e+04  2.064000e+04   \n",
       "mean   4.406467e-17  1.101617e-17  6.885104e-17 -1.018995e-16 -1.514723e-17   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -1.774256e+00 -2.196127e+00 -1.852274e+00 -1.610729e+00 -1.256092e+00   \n",
       "25%   -6.881019e-01 -8.453727e-01 -3.994399e-01 -1.911670e-01 -5.637952e-01   \n",
       "50%   -1.767908e-01  2.864502e-02 -8.078293e-02 -1.010626e-01 -2.291262e-01   \n",
       "75%    4.592952e-01  6.642943e-01  2.519554e-01  6.015724e-03  2.644885e-01   \n",
       "max    5.858144e+00  1.856137e+00  5.516190e+01  6.957003e+01  3.024960e+01   \n",
       "\n",
       "           AveOccup      Latitude     Longitude   MedHouseVal  \n",
       "count  2.064000e+04  2.064000e+04  2.064000e+04  20640.000000  \n",
       "mean   2.754042e-18 -1.035520e-15 -8.526513e-15      2.068558  \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00      1.153956  \n",
       "min   -2.289944e-01 -1.447533e+00 -2.385935e+00      0.149990  \n",
       "25%   -6.170912e-02 -7.967694e-01 -1.113182e+00      1.196000  \n",
       "50%   -2.431526e-02 -6.422715e-01  5.389006e-01      1.797000  \n",
       "75%    2.037404e-02  9.729330e-01  7.784775e-01      2.647250  \n",
       "max    1.194162e+02  2.957996e+00  2.625216e+00      5.000010  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn = df.drop(\n",
    "    columns=target_column_name\n",
    ")  # Eliminar la columna objetivo 'MedHouseVal' del DataFrame y normalizar el resto de columnas\n",
    "dfn = (dfn - dfn.mean()) / dfn.std()  # Normalizamos las columnas\n",
    "\n",
    "dfn[target_column_name] = df[\n",
    "    target_column_name\n",
    "]  # Añadir la columna objetivo 'MedHouseVal' de nuevo al DataFrame normalizado\n",
    "\n",
    "dfn.describe()  # Mostramos un resumen de las estadísticas del DataFrame normalizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset y DataLoader\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "En PyTorch, un `Dataset` es una clase que se encarga de cargar y preparar los datos para su posterior uso en el entrenamiento de modelos. Este objeto almacena los datos y proporciona un método para acceder a ellos de manera eficiente y estructurada. Generalmente, `Dataset` se personaliza según el tipo de datos que se maneja, por ejemplo, imágenes, texto o series temporales. Su implementación básica requiere definir los métodos `__len__` para devolver el tamaño del dataset y `__getitem__` para acceder a un elemento específico.\n",
    "\n",
    "Más información sobre `Dataset` en la documentación oficial de PyTorch: https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset\n",
    "\n",
    "**DataLoader**\n",
    "\n",
    "El `DataLoader` es una clase que se utiliza para envolver un objeto `Dataset` y proporciona un acceso fácil a los datos en lotes durante el entrenamiento, además de otras funcionalidades como el barajado (shuffling) de datos y la carga paralela utilizando múltiples subprocesos. Esto es crucial para el entrenamiento eficiente de modelos, especialmente con grandes volúmenes de datos, ya que gestiona el uso de memoria y mejora el rendimiento computacional.\n",
    "\n",
    "Más información sobre `DataLoader` en la documentación oficial de PyTorch: https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "\n",
    "**Objetivo y Uso**\n",
    "\n",
    "El uso conjunto de `Dataset` y `DataLoader` en PyTorch es fundamental para el flujo de trabajo de entrenamiento de modelos de aprendizaje automático. `Dataset` se encarga de la estructura y accesibilidad de los datos, mientras que `DataLoader` optimiza el proceso de iteración sobre los datos en lotes y facilita operaciones como la mezcla y la carga paralela. Esto permite que el entrenamiento de modelos sea más escalable y eficiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 2 (3895562557.py, line 5)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef __len__(self):\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after function definition on line 2\n"
     ]
    }
   ],
   "source": [
    "class CaliforniaHousingDataset(Dataset):\n",
    "    def __init__(self, dataframe, target_column):\n",
    "        # TODO\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # TODO\n",
    "\n",
    "\n",
    "dataset = CaliforniaHousingDataset(dfn, target_column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el conjunto de datos en tres partes: entrenamiento, validación y prueba. La división se realiza de forma aleatoria, utilizando el 80% de los datos para entrenamiento, el 10% para validación y el 10% restante para pruebas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = (\n",
    "    total_size - train_size - val_size\n",
    ")  # nos aseguramos de que el tamaño del conjunto de test sea correcto\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders\n",
    "\n",
    "Definimos los dataloaders para cada conjunto de datos, estos son los que se encargan de cargar los datos en lotes durante el entrenamiento y la evaluación del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(batch_size, num_workers):\n",
    "\n",
    "    train_loader = # TODO\n",
    "\n",
    "    val_loader = # TODO\n",
    "\n",
    "    test_loader = # TODO\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_data_loaders(BATCH_SIZE, NUM_WORKERS) # obtenemos los dataloaders\n",
    "\n",
    "# probamos un batch del DataLoader\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "print(x_batch.shape, y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pordemos recorrer un barch con un bucle\n",
    "for x_batch, y_batch in train_loader:\n",
    "    print(x_batch.shape, y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo\n",
    "\n",
    "A continuación, definimos un modelo de regresión lineal simple utilizando PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        # TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "input_len = (\n",
    "    len(dfn.columns) - 1\n",
    ")  # Calculamos la cantidad de inputs como la cantidad de columnas -1 (el target)\n",
    "\n",
    "summary(MLP(input_len), input_size=(BATCH_SIZE, input_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Vamos a definir una función de entrenamiento que se encargará de iterar sobre los datos de entrenamiento, calcular las predicciones del modelo, calcular la pérdida y actualizar los pesos del modelo utilizando el optimizador.\n",
    "\n",
    "Tamibén definimos una función de evaluación que se encargará de calcular la pérdida y otras métricas de evaluación en el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, data_loader):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo en los datos proporcionados y calcula la pérdida promedio.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): El modelo que se va a evaluar.\n",
    "        criterion (torch.nn.Module): La función de pérdida que se utilizará para calcular la pérdida.\n",
    "        data_loader (torch.utils.data.DataLoader): DataLoader que proporciona los datos de evaluación.\n",
    "\n",
    "    Returns:\n",
    "        float: La pérdida promedio en el conjunto de datos de evaluación.\n",
    "\n",
    "    \"\"\"\n",
    "    avg_loss = 0\n",
    "    # TODO\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=10,\n",
    "    log_fn=None,\n",
    "    log_every=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Entrena el modelo utilizando el optimizador y la función de pérdida proporcionados.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): El modelo que se va a entrenar.\n",
    "        optimizer (torch.optim.Optimizer): El optimizador que se utilizará para actualizar los pesos del modelo.\n",
    "        criterion (torch.nn.Module): La función de pérdida que se utilizará para calcular la pérdida.\n",
    "        train_loader (torch.utils.data.DataLoader): DataLoader que proporciona los datos de entrenamiento.\n",
    "        val_loader (torch.utils.data.DataLoader): DataLoader que proporciona los datos de validación.\n",
    "        epochs (int): Número de épocas de entrenamiento (default: 10).\n",
    "        log_fn (function): Función que se llamará después de cada log_every épocas con los argumentos (epoch, train_loss, val_loss) (default: None).\n",
    "        log_every (int): Número de épocas entre cada llamada a log_fn (default: 1).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[float], List[float]]: Una tupla con dos listas, la primera con el error de entrenamiento de cada época y la segunda con el error de validación de cada época.\n",
    "\n",
    "    \"\"\"\n",
    "    epoch_train_errors = []  # colectamos el error de traing para posterior analisis\n",
    "    epoch_val_errors = []  # colectamos el error de validacion para posterior analisis\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    return epoch_train_errors, epoch_val_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos hiperparámetros como el número de épocas, la tasa de aprendizaje y la función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los hiperparámetros\n",
    "LR = 0.01\n",
    "CRITERION = nn.MSELoss().to(DEVICE)\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log(epoch, train_loss, val_loss):\n",
    "    print(\n",
    "        f\"Epoch: {epoch + 1:03d}/{EPOCHS:03d} | Train Loss: {train_loss:.5f} | Val Loss: {val_loss:.5f}\"\n",
    "    )\n",
    "\n",
    "model = MLP(input_len).to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR)\n",
    "\n",
    "epoch_train_errors, epoch_val_errors = train(\n",
    "    model, optimizer, CRITERION, train_loader, val_loader, EPOCHS, print_log, 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss durante el entrenamiento\n",
    "\n",
    "Es importante tener en cuenta que la pérdida es una medida de cuán bien se está desempeñando el modelo en el conjunto de entrenamiento. Sin embargo, la pérdida por sí sola no proporciona una visión completa del rendimiento del modelo. Por lo tanto, es fundamental evaluar el modelo en un conjunto de validación independiente para obtener una idea más precisa de su capacidad para generalizar a datos no vistos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_taining(train_errors, val_errors):\n",
    "    # Graficar los errores\n",
    "    plt.figure(figsize=(10, 5))  # Define el tamaño de la figura\n",
    "    plt.plot(train_errors, label=\"Train Loss\")  # Grafica la pérdida de entrenamiento\n",
    "    plt.plot(val_errors, label=\"Validation Loss\")  # Grafica la pérdida de validación\n",
    "    plt.title(\"Training and Validation Loss\")  # Título del gráfico\n",
    "    plt.xlabel(\"Epochs\")  # Etiqueta del eje X\n",
    "    plt.ylabel(\"Loss\")  # Etiqueta del eje Y\n",
    "    plt.legend()  # Añade una leyenda\n",
    "    plt.grid(True)  # Añade una cuadrícula para facilitar la visualización\n",
    "    plt.show()  # Muestra el gráfico\n",
    "\n",
    "\n",
    "plot_taining(epoch_train_errors, epoch_val_errors) # graficamos los errores\n",
    "\n",
    "plot_taining(epoch_train_errors[5:], epoch_val_errors[5:]) # graficamos los errores a partir de la epoca 5 (para ver mejor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight and Bias (W&B)\n",
    "\n",
    "Weight and Bias es una plataforma que facilita el seguimiento, visualización y colaboración en experimentos de aprendizaje automático. Permite registrar métricas, gráficos y modelos de manera centralizada para facilitar el análisis y la comprensión de los resultados obtenidos durante el entrenamiento de modelos.\n",
    "\n",
    "**Funcionalidades Principales**\n",
    "\n",
    "1. **Seguimiento de Experimentos:** Permite registrar métricas clave como pérdida y precisión a lo largo del entrenamiento para cada experimento.\n",
    "\n",
    "2. **Visualización Interactiva:** Proporciona gráficos dinámicos para explorar la evolución de métricas y comparar diferentes experimentos de manera intuitiva.\n",
    "\n",
    "3. **Colaboración y Reproducibilidad:** Facilita compartir resultados con colegas y mantener un registro detallado de todos los experimentos realizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Se requiere una cuenta de W&B, asi como crear un team/equipe y un proyecto para poder utilizar la plataforma. Para más información, visite: https://wandb.ai/site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sweeps en Weight and Bias\n",
    "\n",
    "Uno de los aspectos destacados de Weight and Bias son los \"sweeps\". Estos permiten explorar múltiples combinaciones de hiperparámetros de manera automática, registrando y comparando los resultados de cada configuración. Algunas características de los sweeps incluyen:\n",
    "\n",
    "- **Automatización de Experimentos:** Ejecución paralela de múltiples configuraciones de hiperparámetros para optimizar el rendimiento del modelo.\n",
    "\n",
    "- **Análisis de Resultados:** Visualización automática de los resultados de cada configuración para identificar la mejor combinación de hiperparámetros.\n",
    "\n",
    "- **Ajuste Eficiente:** Permite ajustar rápidamente los hiperparámetros sin necesidad de intervención manual intensiva.\n",
    "\n",
    "Más información sobre Weight and Bias en la documentación oficial: https://docs.wandb.ai/guides/sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_TEAM_NAME = \"[WANDB_TEAM_NAME]\"\n",
    "WANDB_PROJECT = \"[WANDB_PROJECT]\"\n",
    "\n",
    "# La configuración del sweep: los parámetros que queremos optimizar\n",
    "sweep_config = {\n",
    "    \"name\": \"sweep-california-housing\",\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_loss\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\"distribution\": \"uniform\", \"max\": 0.1, \"min\": 0},\n",
    "        \"optimizer\": {\"values\": [\"adam\", \"sgd\"]},\n",
    "        \"batch_size\": {\"values\": [32, 256, 1024, 4096]},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Crea un nuevo sweep (podríamos usar un sweep existente si ya lo hubiéramos creado)\n",
    "sweep_id = wandb.sweep(sweep_config, project=WANDB_PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función Run\n",
    "\n",
    "La función `run` de W&B se utiliza para iniciar un experimento y registrar métricas clave, hiperparámetros y otros detalles relevantes. Esta función se utiliza para rastrear y visualizar los resultados de los experimentos en la plataforma W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wand_log(epoch, train_loss, val_loss):\n",
    "    wandb.log({\"epoch\": epoch + 1, \"train_loss\": train_loss, \"val_loss\": val_loss})\n",
    "\n",
    "\n",
    "def sweep_run():\n",
    "    \"\"\"\n",
    "    Función que se ejecutará en cada run del sweep.\n",
    "    \"\"\"\n",
    "    # inicializar un nuevo run\n",
    "    wandb.init()\n",
    "    # leer la configuración del run\n",
    "    config = wandb.config\n",
    "    run_learning_rate = config.learning_rate\n",
    "    run_optimizer = config.optimizer\n",
    "    run_batch_size = config.batch_size\n",
    "\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corremos el Sweep\n",
    "\n",
    "Para ejecutar un sweep, creamos un agente que le pide a W&B un conjunto de hiperparámetros para cada experimento. Luego, ejecutamos cada experimento con esos hiperparámetros y registramos los resultados en W&B.\n",
    "\n",
    "Es importante que podemos ejecutar en varios entornos, incluso al mismo tiempo, para explorar diferentes configuraciones de hiperparámetros y comparar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, function=sweep_run, count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejor Modelo\n",
    "\n",
    "Una vez que hemos ejecutado el sweep y registrado los resultados en W&B, podemos identificar el mejor modelo basado en las métricas de evaluación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "\n",
    "# nos traemos el sweep (objeto) para analizar los resultados\n",
    "sweep = api.sweep(f\"{WANDB_TEAM_NAME}/{WANDB_PROJECT}/{sweep_id}\")\n",
    "\n",
    "# obtenemos el mejor run\n",
    "best_run = sweep.best_run()\n",
    "\n",
    "# imprimimos el mejor run\n",
    "print(f\"Best run {best_run.name} with {best_run.summary['val_loss']}\")\n",
    "\n",
    "# descargamos el modelo del mejor run\n",
    "best_run.file(\"model.pth\").download(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación final\n",
    "\n",
    "Finalmente, evaluamos el mejor modelo en el conjunto de prueba y visualizamos los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restauramos el modelo\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "# Evaluamos el modelo en el conjunto de test\n",
    "test_loss = evaluate(model, CRITERION, test_loader)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "\n",
    "1. **Expansión de la Configuración del Sweep**:\n",
    "   - **Objetivo**: Ampliar la configuración del sweep existente para incluir la exploración de nuevas arquitecturas de red neuronal.\n",
    "   - **Instrucción**: Expanda el `sweep_config` para incluir parámetros relacionados con la arquitectura del modelo, como el número de capas ocultas (`hidden_layers`) y el número de neuronas por capa (`neurons_per_layer`). Establezca un rango razonable para estos parámetros, por ejemplo, de 1 a 4 para las capas ocultas y de 16 a 128 para las neuronas por capa. Luego, ejecute el sweep y utilice W&B para analizar cómo estos cambios en la arquitectura afectan la métrica de pérdida de validación (`val_loss`).\n",
    "\n",
    "2. **Experimentación con Funciones de Activación**:\n",
    "   - **Objetivo**: Evaluar cómo diferentes funciones de activación afectan el rendimiento del modelo.\n",
    "   - **Instrucción**: Añada una nueva variable a la configuración del sweep para probar diferentes funciones de activación (por ejemplo, ReLU, tanh, sigmoid). Modifique el `sweep_config` para incluir un parámetro `activation_function` con valores posibles como [\"relu\", \"tanh\", \"sigmoid\"]. Ejecute el sweep para explorar cuál función de activación resulta en un mejor rendimiento para el modelo y justifique los resultados basándose en las visualizaciones y métricas registradas en W&B.\n",
    "\n",
    "\n",
    "```python\n",
    "sweep_config = {\n",
    "    \"name\": \"sweep-california-housing-expanded-architecture\",\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_loss\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\"distribution\": \"uniform\", \"max\": 0.1, \"min\": 0},\n",
    "        \"optimizer\": {\"values\": [\"adam\", \"sgd\"]},\n",
    "        \"batch_size\": {\"values\": [32, 256, 1024, 4096]},\n",
    "        \"hidden_layers\": {\"values\": [1, 2, 3, 4]},  # Número de capas ocultas\n",
    "        \"neurons_per_layer\": {\"values\": [16, 32, 64, 128]},  # Número de neuronas por capa\n",
    "        \"activation_function\": {\"values\": [\"relu\", \"tanh\", \"sigmoid\"]}  # Función de activación\n",
    "    },\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taller_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
